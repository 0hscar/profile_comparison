from typing import Generator
from profiles.models.business_profile import BusinessProfile
from profiles.utils.constants import ALLOWED_LLM_MODELS
from core.cache_utils import cache_generator_response, safe_cache_key
import openai
import os
import hashlib
import json

def get_openai_client() -> openai.OpenAI:
    api_key = os.environ.get("OPENAI_API_KEY")
    if not api_key:
        raise RuntimeError("OPENAI_API_KEY environment variable is not set.")
    return openai.OpenAI(api_key=api_key)

def ai_cache_key(question, profile, model="gpt-4.1-mini"):
    base = json.dumps({
        "question": question,
        "profile": profile.json(),
        "model": model
    }, sort_keys=True)
    return safe_cache_key("ai_response:" + hashlib.sha256(base.encode()).hexdigest())

@cache_generator_response(ai_cache_key)
def profile_assistant_response(
    question: str,
    profile: BusinessProfile,
    model: str = "gpt-4.1-mini"
) -> Generator[str, None, None]:
    """
    Stream a conversational, actionable answer to the user's business question.
    Yields chunks of text as they are generated by the AI.
    """
    if model not in ALLOWED_LLM_MODELS:
        raise ValueError(f"Model '{model}' is not in the list of allowed models: {ALLOWED_LLM_MODELS}")

    client = get_openai_client()
    prompt = f"""
    You are a helpful business advisor. Given the following business profile, answer the user's question in a friendly, actionable way.

    Business:
    {profile.json()}

    User question:
    {question}
    """
    stream = client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": "You are a helpful business advisor."},
            {"role": "user", "content": prompt}
        ],
        max_tokens=256,
        temperature=0.7,
        stream=True,
    )
    for chunk in stream:
        if hasattr(chunk.choices[0].delta, "content"):
            yield chunk.choices[0].delta.content
